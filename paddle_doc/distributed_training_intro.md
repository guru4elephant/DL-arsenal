## 分布式训练

### 提升模型训练速度的方法
- 在深度学习模型的设计与训练中，用户可以通过定义数据，定义模型，定义优化目标以及优化算法，快速训练一个模型。以图像分类模型为例，我们在[Imagenet](http://www.image-net.org/)数据集上，基于飞桨编写的[Resnet50](https://arxiv.org/pdf/1512.03385.pdf)模型可以在若干个小时获得业界公认的训练效果。然而，当我们的业务中拥有更多的训练数据，训练Renset50模型至收敛状态可能需要数天时间。想要快速获得较好的模型效果，一种方式就是在单台服务器中使用计算能力更强的AI芯片，另一种方式就是采用多台服务器联合的分布式训练，接下来了几篇文档主要围绕着飞桨的分布式训练能力展开。

### 源自产业实践的飞桨分布式训练
- [PaddlePaddle](https://github.com/PaddlePaddle/Paddle)是百度开源的一款深度学习框架，其中分布式训练能力正是从百度众多大规模深度学习场景对分布式训练的需求中打磨而成，即源于多年的产业实践。在超大规模训练数据很容易获得的场景下，比如推荐、搜索等，用户可以基于海量数据训练效果非常优秀的深度学习模型。为了能够在有限的时间内训练出效果符合预期的模型，并快速应用到产品中，分布式训练的是百度最常见的训练手段。

### 分布式训练的常见方法
- 在深度学习模型的分布式训练中，[数据并行](https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/large_deep_networks_nips2012.pdf)的方法最为常见。数据并行采用将整个训练数据集切分成若干不重叠的分片，分布在多个节点上进行联合训练。为了能够让多个节点能够共享训练的模型参数，分布式训练过程中通常会有一些模型参数同步的机制。[参数服务器](https://www.cs.cmu.edu/~muli/file/parameter_server_osdi14.pdf)即为一种常见的共享模型参数的方式，如图1。参数服务器作为中心化存储模型参数的方法，需要保持与各个训练节点保持频繁通信状态以保证模型的收敛效果。与参数服务器中心化管理模型参数的方法相对应，去中心化的训练方式，通过将模型参数保存在每个训练节点，并通过巧妙的拓扑连接比如[ring-based all reduce](https://github.com/baidu-research/baidu-allreduce)实现模型参数的同步与共享。在分布式训练中，我们通常会为了确定性的训练结果，选择[同步训练](https://openreview.net/pdf?id=D1VDZ5kMAu5jEJ1zfEWL)方式，即各个训练节点的单步执行都会进行全局的参数同步，这样的训练方式与单机训练的算法完全一致。在一些训练数据决定模型效果的应用中，快速的[异步训练](https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/large_deep_networks_nips2012.pdf)也是经常被采用的一种并行训练方式。异步训练通常在参数服务器训练中最为常见，这种情况下每个训练节点与参数服务器同步参数的步调不一致，这给模型训练的收敛性带来了很多挑战，而异步并行优化算法通常也是异步训练场景中经常被研究的重要问题。

### 一些高级的分布式训练模式
- 数据并行是并行训练的最常见方式，然而在一些特殊的模型和特殊的硬件条件下，我们也会需要其他的并行方式。[模型并行]()就是针对模型参数很难在同一块AI芯片上存储和计算而设计的一种并行方式。在模型并行下，神经网络中的一些模型计算单元会被切分为若干可以分治处理的小计算单元，并将小计算单元分配到不同节点上进行计算，并将结果再进行汇总，一个简单的示意图如图3。与数据并行中各个节点进行相同的计算逻辑不同的是，模型并行在每块AI芯片或每个计算节点上训练的模型会有不同。在某些特殊的硬件下，例如同一台服务器拥有CPU、GPU甚至其他类型的芯片的情况下，为了充分利用计算资源，我们会将一个模型的不同部分放到不同的AI芯片中进行计算，由于AI芯片的计算能力有所差别，训练同一个模型需要的各类芯片的数量可能不同，为了能够让不同数量的异构芯片能够高效的协同工作，我们可以采用[流水线并行]()的方式进行模型训练，一个示意图可以参考图。

### 分布式训练分类总结
- 描述了这么多并行训练的方式，我们做一下总结。分布式训练，按照并行的内容来划分，可以分为数据并行、模型并行、流水线并行三种，分别代表着数据的切片，模型的纵向切片，模型的横向切片并行。按照实现的架构来讲，中心化的参数服务器架构和去中心化的集成训练。按照模型参数的同步方法或者分布式优化算法划分，可以分为同步并行训练和异步并行训练。在飞桨中，截止官方发布的1.6版本，数据并行、模型并行、流水线并行已经在框架中支持。GPU芯片下的去中心化同步训练是飞桨重点支持的一种并行方式。大规模CPU集群下经常使用的同步与异步参数服务器架构是飞桨的核心并行能力。

### 飞桨分布式训练API Fleet
- 尽管分布式训练能够大大提升用户在大规模数据下的模型训练速度，将训练程序从单机训练转化为多机训练并不容易，此外多种多样的并行模式也给算法工程师带来了很多选择上的困难。为了降低用户使用分布式训练的门槛，飞桨官方支持分布式训练高级[API Fleet]()，作为分布式训练的统一入口API，用户可以在单机程序的基础上进行简单的几行代码修改即可实现多种类型的并行训练方式。

### 业界分布式训练资源
- [Horovod](https://github.com/horovod/horovod)
- [BytePS](https://github.com/bytedance/byteps)
- [Pytorch Distributed](https://pytorch.org/tutorials/intermediate/dist_tuto.html)
- [TensorFlow Distributed](https://www.tensorflow.org/guide/distributed_training)
