# 分布式训练

### 初识分布式训练
- 在深度学习模型的设计与训练中，用户可以通过定义训练数据，定义模型，定义优化目标以及优化算法，在单台服务器快速训练一个模型。以图像分类模型为例，我们在Imagenet[1]数据集上，基于飞桨编写的Resnet50[2]模型可以在若干个小时获得业界公认的训练效果。然而，当我们的业务中拥有相比Imagenet数据集十倍或百倍的规模时，训练Renset50模型至收敛状态可能需要数天时间。从硬件角度，想要快速获得应用效果不错的模型，一种方式就是在单台服务器中使用计算能力更强的AI芯片，另一种方式就是采用多台服务器联合的分布式训练，接下来了几篇文档主要围绕着飞桨的分布式训练能力展开。分布式训练是海量规模数据下提升训练效率最简单的手段，图1展示了基于飞桨1.6版本训练Resnet50模型在v100 GPU上多机多卡的训练吞吐扩展性，可以看到使用多卡训练可以显著提升训练数据的吞吐。

<p align="center">
<img align="center" src="Resnet50_on_imagenet.png" height="300px" width="510px">
</p>
<p align="center">
图1：Paddle 1.6在Resnet50上的训练吞吐基准
</p>


### 源于产业实践
- [飞桨（PaddlePaddle）](https://github.com/PaddlePaddle/Paddle)是百度开源的一款深度学习框架，其中的分布式训练能力正是从百度众多大规模深度学习场景中打磨而成，可谓源于多年的产业实践。在超大规模训练数据很容易获得的场景下，比如推荐、搜索等，用户可以基于海量数据训练效果非常优秀的深度学习模型。为了能够在有限的时间内训练出效果符合预期的模型，并快速应用到产品中，分布式训练的是百度算法工程师提升训练速度最常见的手段之一。

### 常规训练方法
- 关键词：数据并行、参数服务器、去中心化训练、同步训练、异步训练
- 在深度学习模型的分布式训练中，[数据并行](https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/large_deep_networks_nips2012.pdf)的方法最为常见。数据并行采用将整个训练数据集切分成若干不重叠的分片，分布在多个节点上进行联合训练。为了能够让多个节点能够共享训练的模型参数，分布式训练过程中通常会有一些模型参数同步的机制。[参数服务器](https://www.cs.cmu.edu/~muli/file/parameter_server_osdi14.pdf)即为一种常见的共享模型参数的方式，如图2。参数服务器作为中心化存储模型参数的方法，需要保持与各个训练节点保持频繁通信状态以保证模型的收敛效果。与参数服务器中心化管理模型参数的方法相对应，[去中心化的训练方式]()，通过将模型参数保存在每个训练节点，并通过巧妙的拓扑连接比如[Ring Allreduce](https://github.com/baidu-research/baidu-allreduce)实现模型参数的同步与共享。在分布式训练中，我们通常会为了确定性的训练结果，选择[同步训练](https://openreview.net/pdf?id=D1VDZ5kMAu5jEJ1zfEWL)方式，即各个训练节点的单步执行都会进行全局的参数同步，这样的训练方式与单机训练的算法完全一致。在一些训练数据决定模型效果的应用中，快速的[异步训练](https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/large_deep_networks_nips2012.pdf)也是经常被采用的一种并行训练方式。异步训练通常在参数服务器训练中最为常见，这种情况下每个训练节点与参数服务器同步参数的步调不一致，这给模型训练的收敛性带来了很多挑战，而异步并行优化算法通常也是异步训练场景中经常被研究的重要问题。

<p align="center">
<img align="center" src="data_parallel_ps_collective.png" height="320px" width="840px">
</p>
<p align="center">

### 进阶训练方法
- 关键词：模型并行、流水线并行
- 数据并行是并行训练的最常见方式，然而在一些特殊的模型和特殊的硬件条件下，我们也会需要其他的并行方式。[模型并行]()就是针对模型参数很难在同一块AI芯片上存储和计算而设计的一种并行方式。在模型并行下，神经网络中的一些模型计算单元会被切分为若干可以分治处理的小计算单元，并将小计算单元分配到不同节点上进行计算，并将结果再进行汇总，一个简单的示意图如图3。与数据并行中各个节点进行相同的计算逻辑不同的是，模型并行在每块AI芯片或每个计算节点上训练的模型会有不同。在某些特殊的硬件下，例如同一台服务器拥有CPU、GPU甚至其他类型的芯片的情况下，为了充分利用计算资源，我们会将一个模型的不同部分放到不同的AI芯片中进行计算，由于AI芯片的计算能力有所差别，训练同一个模型需要的各类芯片的数量可能不同，为了能够让不同数量的异构芯片能够高效的协同工作，我们可以采用[流水线并行]()的方式进行模型训练，一个示意图可以参考图。

<p align="center">
<img align="center" src="model_parallel_pipeline_parallel.png" height="320px" width="640px">
</p>
<p align="center">

### 飞桨分布式高级API
- 关键词：[Parameter Server]()，[Collective Training]()
- 尽管分布式训练能够大大提升用户在大规模数据下的模型训练速度，将训练程序从单机训练转化为多机训练并不容易，此外多种多样的并行模式也给算法工程师带来了很多选择上的困难。为了降低用户使用分布式训练的门槛，飞桨官方支持分布式训练高级[API Fleet]()，作为分布式训练的统一入口API，用户可以在单机程序的基础上进行简单的几行代码修改即可实现多种类型的并行训练方式。我们结合Fleet API介绍飞桨的两个主流的并行训练场景[Parameter Server]()和[Collective Training]()，并通过教程的方式给用户一些实际使用的示例。

### 飞桨分布式训练的一些实用的功能
- 关键词：分布式IO、低配网络带宽并行训练、基于重计算的显存优化、基于模型并行的超大规模分类
- 在大规模数据下，数据并行是经常使用的一种训练方式，高性能、多机的数据IO是高效训练的保证，可以参考[Dataset]()获取相关材料。在低配网络场景条件下，GPU多机多卡训练通常受到网络带宽的限制，我们可以使用[适合低配网络环境的分布式训练方式](DGC/GEO/LocalSGD)。很多情况下，AI芯片的显存有限，想要用更大Batch Size数据进行训练，可以采用[重计算]的方法进行训练。在一些大规模类别场景下，比如千万类别、亿级别的分类任务中，单块GPU卡难以满足整个模型的加载，需要使用模型并行的方式进行多卡训练，用户可以参考[PLSC](https://github.com/PaddlePaddle/PLSC)。

### 小结
- 结合以上内容，我们做一下总结。分布式训练，按照并行的内容来划分，可以分为数据并行、模型并行、流水线并行三种，分别代表着数据的切片，模型的纵向切片，模型的横向切片并行。按照实现的架构来讲，中心化的参数服务器架构和去中心化的集成训练。按照模型参数的同步方法或者分布式优化算法划分，可以分为同步并行训练和异步并行训练。在飞桨中，截止官方发布的1.6版本，数据并行、模型并行、流水线并行已经在框架中支持。GPU芯片下的去中心化同步训练是飞桨重点支持的一种并行方式。大规模CPU集群下经常使用的同步与异步参数服务器架构是使用需求非常多的情况。为了帮助用户能够快速上手分布式训练，飞桨提供分布式训练高级API Fleet。在实际使用场景中，例如低配网络集群、超大规模类别分类、显存有限GPU卡等，飞桨也提供了很多实用的分布式训练特性功能。

### 其他开源资源

- [Horovod](https://github.com/horovod/horovod)
- [BytePS](https://github.com/bytedance/byteps)
- [Pytorch Distributed](https://pytorch.org/tutorials/intermediate/dist_tuto.html)
- [TensorFlow Distributed](https://www.tensorflow.org/guide/distributed_training)

### 参考文献

[1]. http://www.image-net.org/

[2]. https://arxiv.org/pdf/1512.03385.pdf
